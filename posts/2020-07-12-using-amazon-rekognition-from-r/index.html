<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Alex Lemm">
    <meta name="description" content="Alex Lemm&#39;s personal website">
    <meta name="keywords" content="blog,analytics,personal,r,datascience,aws">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using Amazon Rekognition from R"/>
<meta name="twitter:description" content="In this article we will demonstrate how to access Amazon Rekognition using the fabulous paws package, an AWS SDK for R, created by David Kretch and Adam Banker.
Amazon Rekognition is a service for image and video analysis which comes with pre-trained deep learning models and a simple API which makes it easy for developers to add these analytics capabilities to their applications.
Some of Amazon Rekognition&rsquo;s features include:
 Object and scene detection Image moderation (checks if the image content is appropriate) Facial analysis Celebrity recognition Face comparison (looking for a facial match in an image based on a single training photo) Text in image (e."/>

    <meta property="og:title" content="Using Amazon Rekognition from R" />
<meta property="og:description" content="In this article we will demonstrate how to access Amazon Rekognition using the fabulous paws package, an AWS SDK for R, created by David Kretch and Adam Banker.
Amazon Rekognition is a service for image and video analysis which comes with pre-trained deep learning models and a simple API which makes it easy for developers to add these analytics capabilities to their applications.
Some of Amazon Rekognition&rsquo;s features include:
 Object and scene detection Image moderation (checks if the image content is appropriate) Facial analysis Celebrity recognition Face comparison (looking for a facial match in an image based on a single training photo) Text in image (e." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://alex23lemm.github.io/posts/2020-07-12-using-amazon-rekognition-from-r/" />
<meta property="article:published_time" content="2020-07-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-07-12T00:00:00+00:00" />


    
      <base href="https://alex23lemm.github.io/posts/2020-07-12-using-amazon-rekognition-from-r/">
    
    <title>
  Using Amazon Rekognition from R · alex23lemm
</title>

    
      <link rel="canonical" href="https://alex23lemm.github.io/posts/2020-07-12-using-amazon-rekognition-from-r/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://alex23lemm.github.io/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    

    

    
    
    <link rel="icon" type="image/png" href="https://alex23lemm.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://alex23lemm.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.80.0" />
  </head>

  <body class=" ">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://alex23lemm.github.io/">
      alex23lemm
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://alex23lemm.github.io/posts/">Blog</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>
<link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark-reasonable.min.css" rel="stylesheet">


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Using Amazon Rekognition from R</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-07-12T00:00:00Z'>
                Jul 12, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              9 minutes read
            </span>
          </div>
          
          
        </div>
      </header>

      <div>
        <p>In this article we will demonstrate how to access Amazon Rekognition using the fabulous <a href="https://github.com/paws-r/paws">paws package</a>, an AWS SDK for R, created by <a href="https://github.com/davidkretch">David Kretch</a> and <a href="https://github.com/adambanker">Adam Banker</a>.</p>
<p><a href="https://aws.amazon.com/rekognition/">Amazon Rekognition</a> is a service for image and video analysis which comes with pre-trained deep learning models and a simple API which makes it easy for developers to add these analytics capabilities to their applications.</p>
<p>Some of Amazon Rekognition&rsquo;s features include:</p>
<ul>
<li>Object and scene detection</li>
<li>Image moderation (checks if the image content is appropriate)</li>
<li>Facial analysis</li>
<li>Celebrity recognition</li>
<li>Face comparison (looking for a facial match in an image based on a single training photo)</li>
<li>Text in image (e.g. useful to analyze text in posted social media images which are usually inaccessible for filters)</li>
</ul>
<p>In the examples below we will focus on how Rekognition&rsquo;s “text-in-image” and “face comparison” features work using R. Following along it will be easy for you to apply your new knowledge to using the other features later on your own.</p>
<p>The entire code of this article is also part of the self-paced and fully reproducible AWS AI Services for R users workshop that you can <a href="https://github.com/alex23lemm/AWS-AI-Services-R-Workshop">download from GitHub here</a>.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>In short, you need to have an IAM user with programmatic access to the AWS API and you need to save the user&rsquo;s access key ID and secret access key as R environment variables:</p>
<p><strong>AWS Cloud</strong></p>
<ul>
<li>
<p>It is a plus if you have some basic familiarity with the AWS Console.</p>
</li>
<li>
<p>You need to have access to an AWS account using an IAM user. <a href="https://aws.amazon.com/free/">AWS Free Tier</a> does also work for this workshop.</p>
</li>
<li>
<p>The IAM user needs to come with <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">security credentials</a> (Access key ID, Secret access key) that allows him to make secure requests to AWS service APIs.</p>
</li>
<li>
<p>You need to make sure that the <code>AmazonRekognitionFullAccess</code> policy is either attached to your IAM user directly or to a group your user is a member of. See <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_change-permissions.html">the official documentation</a> for changing permissions for an IAM user.</p>
</li>
</ul>
<p><strong>Local installations &amp; configuration</strong></p>
<ul>
<li>Install <code>paws</code> from CRAN using <code>install.packages(&quot;paws&quot;) </code> and set the following environment variables in your <code>.Renviron</code> file which is easiest to do using <code>usethis::edit_r_environ()</code>:</li>
</ul>
<pre><code class="language-{r}" data-lang="{r}">AWS_ACCESS_KEY_ID = [YOUR_ACCESS_KEY_ID]
AWS_SECRET_ACCESS_KEY = [YOUR_SECRET_ACCESS_KEY]
AWS_REGION = [CHOOSE_A_REGION_ID_LIKE_us-east-1]
</code></pre><p>Using AWS Rekognition following the examples below should come at no cost. As of writing this article, the service is part of AWS Free Tier, which lasts 12 months and allows you analyze 5,000 images per month and store 1,000 pieces of face metadata per month.</p>
<h2 id="some-additional-information-about-the-api">Some additional information about the API</h2>
<p>You have two options to pass images for their analysis to Rekognition&rsquo;s API endpoints. Either you pass the images as references to images already uploaded in an Amazon S3 bucket. Or you pass the images as raw bytes which means that the images need to reside in your computer&rsquo;s memory.</p>
<p><strong>Important</strong>: Both the paws documentation and the boto3 (official AWS SDK for Python) documentation mention to pass the images as base64-encoded images and not as raw bytes. Unfortunately, it seems that the documentation is not correct in either case because passing base64-encoded images in R did not work for me. <a href="https://stackoverflow.com/questions/47542341/how-do-i-convert-local-jpg-file-to-base64-to-work-with-boto3-and-detect-text">This Stack Overflow post</a>, where someone had the same issue in Python using boto3, made me switch successfully to passing raw bytes instead. So just make sure to think &ldquo;raw bytes&rdquo; every time you read &ldquo;base64-encoded&rdquo; in the paws or boto3 documentation of the Rekognition API.</p>
<p>OK, enough talking! Let&rsquo;s get started.</p>
<h2 id="loading-the-necessary-libraries">Loading the necessary libraries</h2>
<p>First we load the necessary libraries: Using <code>paws</code> is a no brainer here. We will use <code>purrr</code>&rsquo;s magic and <code>tibble</code> to parse the responses of our API requests. We will use <code>readr</code> primarily to read binary data of image files and <code>magick</code> to add Bounding Boxes to images after having received their coordinates via an API response.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">library</span>(paws)
<span style="color:#a6e22e">library</span>(purrr)
<span style="color:#a6e22e">library</span>(tibble)
<span style="color:#a6e22e">library</span>(readr)
<span style="color:#a6e22e">library</span>(magick)
</code></pre></div><h2 id="creating-an-s3-bucket">Creating an S3 bucket</h2>
<p>First we need to create an S3 bucket to which we will upload our image files. We&rsquo;ll create an S3 client and check if any buckets already exist in the region you specified in the <code>.Renviron</code> file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Create S3 client</span>
s3 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">s3</span>()
<span style="color:#75715e"># Let us check if there is an S3 bucket we could use</span>
buckets <span style="color:#f92672">&lt;-</span> s3<span style="color:#f92672">$</span><span style="color:#a6e22e">list_buckets</span>()
<span style="color:#a6e22e">length</span>(buckets<span style="color:#f92672">$</span>Buckets)
</code></pre></div><pre><code>## [1] 0
</code></pre><p>Since there is not yet an S3 bucket available let us create one. Make sure to specify another bucket name since bucket names are globally unique.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">s3<span style="color:#f92672">$</span><span style="color:#a6e22e">create_bucket</span>(Bucket <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;r-and-rekognition-bucket&#34;</span>,
                   CreateBucketConfiguration <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(
                     LocationConstraint <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;eu-central-1&#34;</span>))
</code></pre></div><pre><code>## $Location
## [1] &quot;http://r-and-rekognition-bucket.s3.amazonaws.com/&quot;
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">buckets <span style="color:#f92672">&lt;-</span> s3<span style="color:#f92672">$</span><span style="color:#a6e22e">list_buckets</span>()
buckets <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">map_df</span>(buckets[[1]], 
                  <span style="color:#f92672">~</span><span style="color:#a6e22e">tibble</span>(name <span style="color:#f92672">=</span> .$Name, creationDate <span style="color:#f92672">=</span> .$CreationDate))
buckets
</code></pre></div><pre><code>## # A tibble: 1 x 2
##   name                     creationDate       
##   &lt;chr&gt;                    &lt;dttm&gt;             
## 1 r-and-rekognition-bucket 2020-07-11 21:02:32
</code></pre><p>Let us store the name of our created bucket in a separate variable:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">my_bucket <span style="color:#f92672">&lt;-</span> buckets<span style="color:#f92672">$</span>name[buckets<span style="color:#f92672">$</span>name <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;r-and-rekognition-bucket&#34;</span>]
</code></pre></div><h2 id="text-in-image-detection">Text-in-image detection</h2>
<p>The first feature we will test is text detection in images. This will be our test image:</p>
<p align="center">
<img src="./images/tyrion_quote.jpg" width="40%" />
</p>
<p>As explained above there are two ways to feed a Rekognition endpoint with image data: By referencing images residing in an S3 bucket or by sending the image as raw bytes. Let us check out both options.</p>
<h3 id="option-1-referencing-an-image">Option 1: Referencing an image</h3>
<p>Let us upload the image to the S3 bucket we just created.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">s3<span style="color:#f92672">$</span><span style="color:#a6e22e">put_object</span>(Bucket <span style="color:#f92672">=</span> my_bucket, 
              Body <span style="color:#f92672">=</span> <span style="color:#a6e22e">read_file_raw</span>(<span style="color:#e6db74">&#34;./images/tyrion_quote.jpg&#34;</span>) , 
              Key <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;tyrion_quote.jpg&#34;</span>)
</code></pre></div><pre><code>## $Expiration
## character(0)
## 
## $ETag
## [1] &quot;\&quot;aa5e8918fa049ae75d9b2bb6a484f488\&quot;&quot;
## 
## $ServerSideEncryption
## character(0)
## 
## $VersionId
## character(0)
## 
## $SSECustomerAlgorithm
## character(0)
## 
## $SSECustomerKeyMD5
## character(0)
## 
## $SSEKMSKeyId
## character(0)
## 
## $SSEKMSEncryptionContext
## character(0)
## 
## $RequestCharged
## character(0)
</code></pre><p>OK, that response from the endpoint acknowledging the upload looks rather cryptic. Let us check if our test image now really resides in our bucket.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">bucket_objects <span style="color:#f92672">&lt;-</span> s3<span style="color:#f92672">$</span><span style="color:#a6e22e">list_objects</span>(my_bucket) <span style="color:#f92672">%&gt;%</span> 
  .[[<span style="color:#e6db74">&#34;Contents&#34;</span>]] <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">map_chr</span>(<span style="color:#e6db74">&#34;Key&#34;</span>)
bucket_objects
</code></pre></div><pre><code>## [1] &quot;tyrion_quote.jpg&quot;
</code></pre><p>Perfect, everything worked as expected and our test file was uploaded successfully. Next, we need to create a Rekognition client for making the necessary API calls:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Create a Rekognition client</span>
rekognition <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rekognition</span>()

<span style="color:#75715e"># Referencing an image in an Amazon S3 bucket</span>
resp <span style="color:#f92672">&lt;-</span> rekognition<span style="color:#f92672">$</span><span style="color:#a6e22e">detect_text</span>(
  Image <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(
    S3Object <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(
      Bucket <span style="color:#f92672">=</span> my_bucket,
      Name <span style="color:#f92672">=</span> bucket_objects
    )
  )
)

<span style="color:#75715e"># Parsing the response</span>
resp <span style="color:#f92672">%&gt;%</span> 
  .[[<span style="color:#e6db74">&#34;TextDetections&#34;</span>]] <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">keep</span>(<span style="color:#f92672">~</span>.[[<span style="color:#e6db74">&#34;Type&#34;</span>]] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;WORD&#34;</span>) <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">map_chr</span>(<span style="color:#e6db74">&#34;DetectedText&#34;</span>)
</code></pre></div><pre><code>##  [1] &quot;THAT'S&quot; &quot;WHAT&quot;   &quot;I&quot;      &quot;DO&quot;     &quot;I&quot;      &quot;DRINK&quot;  &quot;AND&quot;    &quot;I&quot;     
##  [9] &quot;KNOW&quot;   &quot;THINGS&quot;
</code></pre><p>Hurray! We used Rekognition the first time and the service delivered the correct results. What about sending the image in raw format instead?</p>
<h3 id="option-2-sending-the-image-as-bytes">Option 2: Sending the image as bytes</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">image <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">read_file_raw</span>(<span style="color:#e6db74">&#34;./images/tyrion_quote.jpg&#34;</span>)

<span style="color:#75715e"># Sending the image as raw bytes</span>
resp <span style="color:#f92672">&lt;-</span> rekognition<span style="color:#f92672">$</span><span style="color:#a6e22e">detect_text</span>(
  Image <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(
    Bytes <span style="color:#f92672">=</span> image
  )
)

<span style="color:#75715e"># Parsing the response</span>
resp <span style="color:#f92672">%&gt;%</span> 
  .[[<span style="color:#e6db74">&#34;TextDetections&#34;</span>]] <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">keep</span>(<span style="color:#f92672">~</span>.[[<span style="color:#e6db74">&#34;Type&#34;</span>]] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;WORD&#34;</span>) <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">map_chr</span>(<span style="color:#e6db74">&#34;DetectedText&#34;</span>)
</code></pre></div><pre><code>##  [1] &quot;THAT'S&quot; &quot;WHAT&quot;   &quot;I&quot;      &quot;DO&quot;     &quot;I&quot;      &quot;DRINK&quot;  &quot;AND&quot;    &quot;I&quot;     
##  [9] &quot;KNOW&quot;   &quot;THINGS&quot;
</code></pre><p>The second option also worked perfectly fine and delivered the same results. In the next section we will move on to test the other feature we are interested in.</p>
<h3 id="facial-comparison">Facial comparison</h3>
<p>Facial comparison lets you look for a facial match in an image based on a single training photo you provided. It is best explained with a simple example.</p>
<p>Imagine the security camera caught a thief stealing your car in front of your house. The guy looks like this:</p>
<p align="center">
<img src="./images/thief.jpg" width="30%" />
</p>
<p>Rekognition can now be used to identify the thief in a photo of a group of people. By coincidence the police acquired a photo of the usual suspects who had a Web session together the same day in the area the car was stolen:</p>
<p align="center">
<img src="images/usual_suspects.png" width="60%" />
</p>
<p>Is our thief among these people and can Rekognition identify him correctly?</p>
<p>We will skip uploading the two images to S3 and send them directly as raw bytes to the API endpoint. Uploading the images to S3 and referencing them for a facial comparison would work in the same manner as described above in the text-in-image detection demo.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Load raw images into memory</span>
thief <span style="color:#f92672">&lt;-</span> readr<span style="color:#f92672">::</span><span style="color:#a6e22e">read_file_raw</span>(<span style="color:#e6db74">&#34;./images/thief.jpg&#34;</span>)
suspects <span style="color:#f92672">&lt;-</span> readr<span style="color:#f92672">::</span><span style="color:#a6e22e">read_file_raw</span>(<span style="color:#e6db74">&#34;./images/usual_suspects.png&#34;</span>)

<span style="color:#75715e"># Send images to the compare faces endpoint</span>
resp <span style="color:#f92672">&lt;-</span> rekognition<span style="color:#f92672">$</span><span style="color:#a6e22e">compare_faces</span>(
  SourceImage <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(
    Bytes <span style="color:#f92672">=</span> thief
  ),
  TargetImage <span style="color:#f92672">=</span> <span style="color:#a6e22e">list</span>(
    Bytes <span style="color:#f92672">=</span> suspects
  )
)
</code></pre></div><p>Did the response include a hit?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">length</span>(resp<span style="color:#f92672">$</span>UnmatchedFaces)
</code></pre></div><pre><code>## [1] 15
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">length</span>(resp<span style="color:#f92672">$</span>FaceMatches)
</code></pre></div><pre><code>## [1] 1
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">resp<span style="color:#f92672">$</span>FaceMatches[[1]]<span style="color:#f92672">$</span>Similarity
</code></pre></div><pre><code>## [1] 99.6824
</code></pre><p>OK, it seems Rekognition identified one of the 16 persons in the suspects image as a potential hit and also seems 99.68 % certain about it.</p>
<p>The response of Rekognition&rsquo;s compare faces endpoint also includes bounding box coordinates for items that are detected in images (See the <a href="https://docs.aws.amazon.com/rekognition/latest/dg/images-displaying-bounding-boxes.html">official documentation here)</a>.</p>
<p>We will use the <code>magick</code> package to add the bounding box of the person who Rekognition identified as the thief to the image of the suspects.</p>
<p>For this we need to convert the suspects raw image which currently resides in our local memory into a magick image object. This step is necessary because we need to extract the pixel information of the suspect image using <code>magick::image_info()</code>. We will use the pixel information  together with the bounding box coordinates included in the response to calculate the bounding box properties (width, height, left, right).</p>
<p>Once we have the bounding box properties we can add the bounding box to our suspects image using <code>magick</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Convert raw image into a magick object</span>
suspects <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">image_read</span>(suspects)

<span style="color:#75715e"># Extract face match from the response</span>
match <span style="color:#f92672">&lt;-</span> resp<span style="color:#f92672">$</span>FaceMatches[[1]]

<span style="color:#75715e"># Calculate bounding box properties</span>
width <span style="color:#f92672">&lt;-</span> match<span style="color:#f92672">$</span>Face<span style="color:#f92672">$</span>BoundingBox<span style="color:#f92672">$</span>Width <span style="color:#f92672">*</span> <span style="color:#a6e22e">image_info</span>(suspects)<span style="color:#f92672">$</span>width 
height <span style="color:#f92672">&lt;-</span> match<span style="color:#f92672">$</span>Face<span style="color:#f92672">$</span>BoundingBox<span style="color:#f92672">$</span>Height <span style="color:#f92672">*</span> <span style="color:#a6e22e">image_info</span>(suspects)<span style="color:#f92672">$</span>height
left <span style="color:#f92672">&lt;-</span> match<span style="color:#f92672">$</span>Face<span style="color:#f92672">$</span>BoundingBox<span style="color:#f92672">$</span>Left <span style="color:#f92672">*</span> <span style="color:#a6e22e">image_info</span>(suspects)<span style="color:#f92672">$</span>width 
top <span style="color:#f92672">&lt;-</span> match<span style="color:#f92672">$</span>Face<span style="color:#f92672">$</span>BoundingBox<span style="color:#f92672">$</span>Top <span style="color:#f92672">*</span> <span style="color:#a6e22e">image_info</span>(suspects)<span style="color:#f92672">$</span>height

<span style="color:#75715e"># Add bounding box to suspects image</span>
image <span style="color:#f92672">&lt;-</span> suspects <span style="color:#f92672">%&gt;%</span> 
  <span style="color:#a6e22e">image_draw</span>()
<span style="color:#a6e22e">rect</span>(left, top, left <span style="color:#f92672">+</span> width, top <span style="color:#f92672">+</span> height, border <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;red&#34;</span>, lty <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;dashed&#34;</span>, lwd <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>)
image
</code></pre></div><p align="center">
<img src="./images/unnamed-chunk-14-1.png" width="60%" style="display: block; margin: auto;" />
</p>
<p>Second Hurray! Testing Rekognition&rsquo;s face comparison feature was also a success and the thief was identified correctly.</p>
<h2 id="summary">Summary</h2>
<p>It is great that Python&rsquo;s <code>boto3</code> finally has an R brother. The <code>paws</code> package makes it really easy to access Amazon Rekognition and other AWS services from R. R users can use image and video analysis capabilities without being experts in deep learning. The trickiest part when using <code>paws</code> is figuring out a clever way to parse the response coming from the API. But with the recipes and packages used above for parsing S3 and Rekognition API responses, you now should have have some best practices available on your end to get started quickly.</p>

      </div>

      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "alex23lemm-github-io-1" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
     © 2021
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
    
  </section>
</footer>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>

    </main>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-55248554-4', 'auto');
	
	ga('send', 'pageview');
}
</script>


  </body>

</html>
